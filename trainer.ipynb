{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Academic Paper PDF Summarizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF Processing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP and Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import (\n",
    "    BartTokenizer, \n",
    "    BartForConditionalGeneration,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Preprocessing\n",
    "\n",
    "### PDF Extraction Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path: Union[str, Path]) -> str:\n",
    "  \"\"\"\n",
    "  Extract text from a PDF file using PyPDF2.\n",
    "\n",
    "  Args: \n",
    "    pdf_path (Union[str, Path]): Path to the PDF file\n",
    "  \n",
    "  Returns:\n",
    "    str: Extracted text from the PDF\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "      reader = PdfReader(file)\n",
    "      text = \"\"\n",
    "\n",
    "      for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "\n",
    "        if page_text:\n",
    "          text += page_text + \"\\n\"\n",
    "        \n",
    "        return text\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f\"Error extracting text from PDF: {e}\")\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning and Section Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_academic_paper(text: str) -> str:\n",
    "  \"\"\"\n",
    "  Clean extracted text from academic papers\n",
    "\n",
    "  Args:\n",
    "    text (str): Extracted text from PDF\n",
    "\n",
    "  Returns:\n",
    "    str: Cleaned text\n",
    "  \"\"\"\n",
    "\n",
    "  text = re.sub(r'\\n\\s*\\d+\\s*\\n', '\\n', text)\n",
    "\n",
    "  # Remove headers and footers\n",
    "  lines = text.split('\\n')\n",
    "  filtered_lines = []\n",
    "  header_footer_patterns = set()\n",
    "\n",
    "  line_counts = {}\n",
    "  for line in lines:\n",
    "    line = line.strip()\n",
    "\n",
    "    if line:\n",
    "      if line in line_counts:\n",
    "        line_counts[line] += 1\n",
    "\n",
    "      else:\n",
    "        line_counts[line] = 1\n",
    "\n",
    "  # Consider lines appearing more than twice as header/footers\n",
    "  for line, count in line_counts.items():\n",
    "    if count > 2 and len(line) < 100:\n",
    "      header_footer_patterns.add(line)\n",
    "\n",
    "  # Filter out headers/footers\n",
    "  for line in lines:\n",
    "    if line.strip() not in header_footer_patterns:\n",
    "      filtered_lines.append(line)\n",
    "\n",
    "  cleaned_text = \"\\n\".join(filtered_lines)\n",
    "  cleaned_text = re.sub(r\" +\", ' ', cleaned_text)\n",
    "  cleaned_text = re.sub(r\"\\n+\", '\\n', cleaned_text)\n",
    "\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sections(text: str) -> dict:\n",
    "  \"\"\"\n",
    "  Extract main sections from an academic paper\n",
    "\n",
    "  Args:\n",
    "    text (str): Cleaned text from academic paper\n",
    "\n",
    "  Returns:\n",
    "    dict: Dictionary containing sections\n",
    "  \"\"\"\n",
    "\n",
    "  section_patterns = {\n",
    "        'abstract': r'abstract',\n",
    "        'introduction': r'introduction|background',\n",
    "        'methodology': r'methodology|method|materials and methods|experimental',\n",
    "        'results': r'results|findings',\n",
    "        'discussion': r'discussion',\n",
    "        'conclusion': r'conclusion|summary|future work'\n",
    "    }\n",
    "  \n",
    "  sections = {}\n",
    "\n",
    "  lines = text.split(\"\\n\")\n",
    "  current_section = 'preamble'\n",
    "  sections[current_section] = []\n",
    "\n",
    "  for line in lines:\n",
    "    section_found = False\n",
    "\n",
    "    for section_name, pattern in section_patterns.items():\n",
    "      if re.search(pattern, line.lower()):\n",
    "        current_section = section_name\n",
    "        sections[current_section] = []\n",
    "        section_found = True\n",
    "\n",
    "        break\n",
    "\n",
    "      if not section_found:\n",
    "        sections[current_section].append(line)\n",
    "\n",
    "  for section in sections:\n",
    "    sections[section] = '\\n'.join(sections[section])\n",
    "\n",
    "  return sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading PDFs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_papers_from_directory(directory_path: Union[str, Path]):\n",
    "  \"\"\"\n",
    "  Load and process academic papers from a directory\n",
    "\n",
    "  Args:\n",
    "    directory_path (Union[str, Path]): Path to directory containing PDFs\n",
    "\n",
    "  Returns:\n",
    "    list: List of processed paper\n",
    "  \"\"\"\n",
    "\n",
    "  paper_files = [f for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "  papers = []\n",
    "\n",
    "  for paper_file in tqdm(paper_files, desc=\"Processing PDFs\"):\n",
    "    paper_path = os.path.join(directory_path, paper_file)\n",
    "    text = extract_text_from_pdf(paper_path)\n",
    "\n",
    "    if text:\n",
    "      cleaned_text = clean_academic_paper(text)\n",
    "      sections = extract_sections(cleaned_text)\n",
    "\n",
    "      papers.append({\n",
    "        'filename': paper_file,\n",
    "        'full_text': cleaned_text,\n",
    "        'sections': sections,\n",
    "        'summary': sections.get('abstract', '')\n",
    "      })\n",
    "\n",
    "  return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Architecture and Dataset Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Custom Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcademicPaperDataset(Dataset):\n",
    "  def __init__(self, papers, summaries, tokenizer, max_input_length: int = 1024, max_target_length: int = 256):\n",
    "    self.papers = papers\n",
    "    self.summaries = summaries\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_input_length = max_input_length\n",
    "    self.max_target_length = max_target_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.papers)\n",
    "  \n",
    "  def __getitem__(self, idx: int):\n",
    "    paper = self.papers[idx]\n",
    "    summary = self.summaries[idx]\n",
    "\n",
    "    inputs = self.tokenizer(\n",
    "      paper,\n",
    "      max_length=self.max_input_length,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    targets = self.tokenizer(\n",
    "      summary,\n",
    "      max_length=self.max_input_length,\n",
    "      padding=\"max_length\",\n",
    "      truncation=True,\n",
    "      return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = inputs.input_ids.squeeze()\n",
    "    attention_mask = inputs.attention_mask.squeeze()\n",
    "    labels = targets.inputs_ids.squeeze()\n",
    "\n",
    "    labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"attention_mask\": attention_mask,\n",
    "      \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Initialize Model and Tokenizer Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(model_type=\"bart\", pretrained_model=None):\n",
    "  \"\"\"\n",
    "  Initialize a summarization model and tokenizer\n",
    "\n",
    "  Args:\n",
    "    model_type (str): Type of model to use (\"bart\" or \"t5\")\n",
    "    pretrained_model (str): Path to pretrained model (optional)\n",
    "\n",
    "  Returns:\n",
    "    tuple: (model, tokenizer)\n",
    "  \"\"\"\n",
    "\n",
    "  if model_type.lower() == \"bart\":\n",
    "    model_name = \"facebook/bart-large-cnn\"\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if pretrained_model:\n",
    "      model = BartForConditionalGeneration.from_pretrained(pretrained_model)\n",
    "\n",
    "    else:\n",
    "      model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "  elif model_type.lower() == \"t5\":\n",
    "    model_name = \"t5-base\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if pretrained_model:\n",
    "      model = T5ForConditionalGeneration.from_pretrained(pretrained_model)\n",
    "\n",
    "    else:\n",
    "      model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "  else:\n",
    "    raise ValueError(f\"Unsupported model type: {model_type}. Use 'bart' or 't5'.\")\n",
    "  \n",
    "  model = model.to(device)\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, tokenizer,\n",
    "                epochs=3, lr=2e-5, warmup_steps=500,\n",
    "                eval_steps=100, save_path=\"model_checkpoints\"):\n",
    "  \"\"\"\n",
    "  Train the summarization model\n",
    "\n",
    "  Args:\n",
    "        model: The model to train\n",
    "        train_dataloader: DataLoader for training data\n",
    "        val_dataloader: DataLoader for validation data\n",
    "        tokenizer: Tokenizer for decoding predictions\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        warmup_steps: Number of warmup steps for scheduler\n",
    "        eval_steps: Evaluate model every eval_steps steps\n",
    "        save_path: Path to save model checkpoints\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (Trained model, training history)\n",
    "  \"\"\"\n",
    "\n",
    "  os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "  optimizer = AdamW(model.parameters(), lr=lr)\n",
    "  total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "  scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    "  )\n",
    "\n",
    "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "  history = {\n",
    "    'train_loss': [],\n",
    "    'val_rouge1': [],\n",
    "    'val_rouge2': [],\n",
    "    'val_rougeL': []\n",
    "  }\n",
    "\n",
    "  global_step = 0\n",
    "  best_rouge_l = 0.0\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} / {epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Training epoch {epoch + 1}\")\n",
    "\n",
    "    for batch in progress_bar:\n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "      attention_mask = batch[\"attention_mask\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      # Clear previous gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Forward pass\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels\n",
    "      )\n",
    "\n",
    "      loss = outputs.loss\n",
    "      epoch_loss += loss.item()\n",
    "\n",
    "      # Updates\n",
    "      progress_bar.set_postfix({'loss': loss.item()})\n",
    "      loss.backward()\n",
    "\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "\n",
    "      global_step += 1\n",
    "\n",
    "      # Evaluate during training\n",
    "      if global_step % eval_steps == 0:\n",
    "        print(f\"\\nEvaluating at step {global_step}\")\n",
    "        rouge_scores = evaluate_model(model, val_dataloader, tokenizer, scorer)\n",
    "\n",
    "        # Print scores\n",
    "        print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
    "        print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
    "        print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "\n",
    "        # Update history\n",
    "        history['val_rouge1'].append((global_step, rouge_scores['rouge1']))\n",
    "        history['val_rouge2'].append((global_step, rouge_scores['rouge2']))\n",
    "        history['val_rougeL'].append((global_step, rouge_scores['rougeL']))\n",
    "\n",
    "        # Save best model\n",
    "        if rouge_scores[\"rougeL\"] > best_rouge_l:\n",
    "          best_rouge_l = rouge_scores['rougeL']\n",
    "          model_path = os.path.join(save_path, f'best_model_step_{global_step}.pt')\n",
    "          torch.save(model.state_dict(), model_path)\n",
    "          print(f\"Saved best model to {model_path}\")\n",
    "                \n",
    "          # Switch back to training mode\n",
    "          model.train()\n",
    "\n",
    "      avg_loss = epoch_loss / len(train_dataloader)\n",
    "      history['train_loss'].append((global_step, avg_loss))\n",
    "      print(f\"Average training loss: {avg_loss:.4f}\")\n",
    "\n",
    "      # Save epoch checkpoint\n",
    "      model_path = os.path.join(save_path, f'model_epoch_{epoch+1}.pt')\n",
    "      torch.save(model.state_dict(), model_path)\n",
    "      print(f\"Saved epoch checkpoint to {model_path}\")\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluation Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, tokenizer, scorer=None):\n",
    "  \"\"\"\n",
    "  Evaluate the model using ROUGE scores.\n",
    "  \n",
    "  Args:\n",
    "      model: The model to evaluate\n",
    "      dataloader: DataLoader with evaluation data\n",
    "      tokenizer: Tokenizer for decoding predictions\n",
    "      scorer: Rouge scorer (if None, will create a new one)\n",
    "      \n",
    "  Returns:\n",
    "      dict: Dictionary with ROUGE scores\n",
    "  \"\"\"\n",
    "  \n",
    "  model.eval()\n",
    "  if scorer is None:\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    \n",
    "  rouge_scores = []\n",
    "  with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch[\"attention_mask\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      summary_ids = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=150,\n",
    "        min_length=40,\n",
    "        length_penalty=2.0,\n",
    "        num_beams=4,\n",
    "        early_stopping=True\n",
    "      )\n",
    "\n",
    "      # Convert to text\n",
    "      predictions = [tokenizer.decode(g, skip_special_tokens=True) for g in summary_ids]\n",
    "\n",
    "      # Get reference summaries (labels)\n",
    "      references = []\n",
    "      for label in labels:\n",
    "        label = torch.where(label != -100, label, tokenizer.pad_token_id)\n",
    "        references.append(tokenizer.decode(label, skip_special_tokens=True))\n",
    "\n",
    "      # Calculate ROUGE scores\n",
    "      for pred, ref in zip(predictions, references):\n",
    "        score = scorer.score(ref, pred)\n",
    "        rouge_scores.append(score)\n",
    "\n",
    "  # Calculate avg scores\n",
    "  avg_rouge1 = sum(score['rouge1'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
    "  avg_rouge2 = sum(score['rouge2'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
    "  avg_rougeL = sum(score['rougeL'].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
    "    \n",
    "  return {\n",
    "    'rouge1': avg_rouge1,\n",
    "    'rouge2': avg_rouge2,\n",
    "    'rougeL': avg_rougeL\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Long Document Handling Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_long_document(document, model, tokenizer, max_length=1024, overlap=256):\n",
    "  \"\"\"\n",
    "  Process a long document by splitting it into chunks with overlap\n",
    "  \"\"\"\n",
    "\n",
    "  model.eval()\n",
    "  tokens = tokenizer.encode(document)\n",
    "\n",
    "  if len(tokens) <= max_length:\n",
    "    inputs = tokenizer(document, max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    summary_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      attention_mask=inputs[\"attention_masks\"],\n",
    "      max_length=150,\n",
    "      min_length=40,\n",
    "      length_penalty=2.0,\n",
    "      num_beams=4,\n",
    "      early_stopping=True\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "  \n",
    "  chunks = []\n",
    "  for i in range(0, len(tokens), max_length - overlap):\n",
    "    chunk = tokens[i : i + max_length]\n",
    "    chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))\n",
    "\n",
    "  chunk_summaries = []  \n",
    "  for chunk in chunks:\n",
    "    inputs = tokenizer(chunk, max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    summary_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"],\n",
    "      attention_mask=inputs[\"attention_masks\"],\n",
    "      max_length=100,\n",
    "      min_length=20,\n",
    "      length_penalty=2.0,\n",
    "      num_beams=4,\n",
    "      early_stopping=True\n",
    "    )\n",
    "\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    chunk_summaries.append(summary)\n",
    "\n",
    "  combined_summary = \" \".join(chunk_summaries)\n",
    "  inputs = tokenizer(combined_summary, max_length, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "  final_summary_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_masks\"],\n",
    "    max_length=150,\n",
    "    min_length=40,\n",
    "    length_penalty=2.0,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    "  )\n",
    "\n",
    "  final_summary = tokenizer.decode(final_summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "  return final_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualizing Training Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, save_path=None):\n",
    "  plt.figure(figsize=(12, 8))\n",
    "\n",
    "  # Plot training loss\n",
    "  plt.subplot(2, 1, 1)\n",
    "  steps, losses = zip(*history['train_loss'])\n",
    "  plt.plot(steps, losses)\n",
    "  plt.title('Training Loss')\n",
    "  plt.xlabel('Steps')\n",
    "  plt.ylabel('Loss')\n",
    "  \n",
    "  # Plot ROUGE scores\n",
    "  plt.subplot(2, 1, 2)\n",
    "  steps_rouge1, rouge1_scores = zip(*history['val_rouge1'])\n",
    "  steps_rouge2, rouge2_scores = zip(*history['val_rouge2'])\n",
    "  steps_rougeL, rougeL_scores = zip(*history['val_rougeL'])\n",
    "  \n",
    "  plt.plot(steps_rouge1, rouge1_scores, label='ROUGE-1')\n",
    "  plt.plot(steps_rouge2, rouge2_scores, label='ROUGE-2')\n",
    "  plt.plot(steps_rougeL, rougeL_scores, label='ROUGE-L')\n",
    "  plt.title('ROUGE Scores during Training')\n",
    "  plt.xlabel('Steps')\n",
    "  plt.ylabel('Score')\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.tight_layout()\n",
    "  \n",
    "  if save_path:\n",
    "      plt.savefig(save_path)\n",
    "  \n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_academic_summarizer(data_path, output_path, model_type='bart', epochs=3, batch_size=2, max_length=1024):\n",
    "  \"\"\"\n",
    "  Complete workflow for training an academic paper summarizer.\n",
    "  \n",
    "  Args:\n",
    "      data_path (str): Path to directory containing PDF files\n",
    "      output_path (str): Path to save model and results\n",
    "      model_type (str): Type of model to use ('bart' or 't5')\n",
    "      epochs (int): Number of training epochs\n",
    "      batch_size (int): Batch size for training\n",
    "      max_length (int): Maximum input sequence length\n",
    "  \"\"\"\n",
    "\n",
    "  os.makedirs(output_path, exist_ok=True)\n",
    "  papers = load_papers_from_directory(data_path)\n",
    "\n",
    "  if not papers:\n",
    "    print(\"No papers found or processed. Check the data path.\")\n",
    "    return\n",
    "  \n",
    "  # Prepare data from training\n",
    "  paper_texts = [paper['full_text'] for paper in papers]\n",
    "  paper_summaries = [paper['summary'] for paper in papers]\n",
    "\n",
    "  train_texts, val_texts, train_summaries, val_summaries = train_test_split(\n",
    "    paper_texts, paper_summaries, test_size=0.2, random_state=42\n",
    "  )\n",
    "\n",
    "  print(f\"Training set: {len(train_texts)} papers\")\n",
    "  print(f\"Validation set: {len(val_texts)} papers\")\n",
    "\n",
    "  # Initialize the model and tokenizer\n",
    "  model, tokenizer = initialize_model(model_type=model_type)\n",
    "\n",
    "  # Create datasets and dataloaders\n",
    "  train_dataset = AcademicPaperDataset(\n",
    "    train_texts, train_summaries, tokenizer, max_input_length=max_length\n",
    "  )\n",
    "\n",
    "  val_dataset = AcademicPaperDataset(\n",
    "    val_texts, val_summaries, tokenizer, max_input_length=max_length\n",
    "  )\n",
    "\n",
    "  train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "  val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "  # Train the model\n",
    "  model_save_path = os.path.join(output_path, 'model_checkpoints')\n",
    "  trained_model, history = train_model(\n",
    "    model=model,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    tokenizer=tokenizer,\n",
    "    epochs=epochs,\n",
    "    save_path=model_save_path\n",
    "  )\n",
    "\n",
    "  # Plot history \n",
    "  history_plot_path = os.path.join(output_path, 'training_history.png')\n",
    "  plot_training_history(history, save_path=history_plot_path)\n",
    "\n",
    "  print(\"Final evaluation on validation set...\")\n",
    "  final_scores = evaluate_model(trained_model, val_dataloader, tokenizer)\n",
    "  print(f\"Final ROUGE-1: {final_scores['rouge1']:.4f}\")\n",
    "  print(f\"Final ROUGE-2: {final_scores['rouge2']:.4f}\")\n",
    "  print(f\"Final ROUGE-L: {final_scores['rougeL']:.4f}\")\n",
    "  \n",
    "  # 8. Save evaluation results\n",
    "  with open(os.path.join(output_path, 'evaluation_results.txt'), 'w') as f:\n",
    "      f.write(f\"Model type: {model_type.upper()}\\n\")\n",
    "      f.write(f\"Training epochs: {epochs}\\n\")\n",
    "      f.write(f\"Training set size: {len(train_texts)}\\n\")\n",
    "      f.write(f\"Validation set size: {len(val_texts)}\\n\")\n",
    "      f.write(f\"Final ROUGE-1: {final_scores['rouge1']:.4f}\\n\")\n",
    "      f.write(f\"Final ROUGE-2: {final_scores['rouge2']:.4f}\\n\")\n",
    "      f.write(f\"Final ROUGE-L: {final_scores['rougeL']:.4f}\\n\")\n",
    "  \n",
    "  print(f\"Training completed. Model saved to {model_save_path}\")\n",
    "  print(f\"Results saved to {output_path}\")\n",
    "  \n",
    "  return trained_model, tokenizer, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path, model_type='bart'):\n",
    "  \"\"\"\n",
    "  Load a trained summarization model.\n",
    "  \n",
    "  Args:\n",
    "      model_path (str): Path to the saved model\n",
    "      model_type (str): Type of model ('bart' or 't5')\n",
    "      \n",
    "  Returns:\n",
    "      tuple: (model, tokenizer)\n",
    "  \"\"\"\n",
    "  \n",
    "  model, tokenizer = initialize_model(model_type)\n",
    "  model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "\n",
    "  return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_paper(pdf_path, model, tokenizer):\n",
    "  \"\"\"\n",
    "  Summarize a single academic paper.\n",
    "  \n",
    "  Args:\n",
    "      pdf_path (str): Path to the PDF file\n",
    "      model: The summarization model\n",
    "      tokenizer: The tokenizer\n",
    "      \n",
    "  Returns:\n",
    "      dict: Dictionary with original text, sections, and summary\n",
    "  \"\"\"\n",
    "  # Extract text from PDF\n",
    "  text = extract_text_from_pdf(pdf_path)\n",
    "  \n",
    "  if not text:\n",
    "      return {\"error\": \"Failed to extract text from PDF\"}\n",
    "  \n",
    "  # Clean the text\n",
    "  cleaned_text = clean_academic_paper(text)\n",
    "  sections = extract_sections(cleaned_text)\n",
    "  summary = process_long_document(cleaned_text, model, tokenizer)\n",
    "  \n",
    "  return {\n",
    "    \"filename\": os.path.basename(pdf_path),\n",
    "    \"full_text\": cleaned_text,\n",
    "    \"sections\": sections,\n",
    "    \"summary\": summary\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_summarize_papers(pdf_directory, model, tokenizer, output_directory=None):\n",
    "  \"\"\"\n",
    "  Summarize multiple papers in a directory.\n",
    "  \n",
    "  Args:\n",
    "      pdf_directory (str): Path to directory containing PDFs\n",
    "      model: The summarization model\n",
    "      tokenizer: The tokenizer\n",
    "      output_directory (str): Path to save summaries (optional)\n",
    "      \n",
    "  Returns:\n",
    "      list: List of dictionaries with summaries\n",
    "  \"\"\"\n",
    "  if output_directory:\n",
    "      os.makedirs(output_directory, exist_ok=True)\n",
    "  \n",
    "  pdf_files = [f for f in os.listdir(pdf_directory) if f.endswith('.pdf')]\n",
    "  summaries = []\n",
    "  \n",
    "  for pdf_file in tqdm(pdf_files, desc=\"Summarizing papers\"):\n",
    "      pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "      \n",
    "      # Summarize the paper\n",
    "      result = summarize_paper(pdf_path, model, tokenizer)\n",
    "      summaries.append(result)\n",
    "      \n",
    "      # Save the summary if output directory is provided\n",
    "      if output_directory and 'error' not in result:\n",
    "          output_file = os.path.join(output_directory, f\"{os.path.splitext(pdf_file)[0]}_summary.txt\")\n",
    "          with open(output_file, 'w', encoding='utf-8') as f:\n",
    "              f.write(f\"# Summary of {pdf_file}\\n\\n\")\n",
    "              f.write(result['summary'])\n",
    "              f.write(\"\\n\\n# Extracted Sections\\n\\n\")\n",
    "              for section_name, section_text in result['sections'].items():\n",
    "                  if section_text.strip():\n",
    "                      f.write(f\"## {section_name.capitalize()}\\n\\n\")\n",
    "                      f.write(section_text)\n",
    "                      f.write(\"\\n\\n\")\n",
    "  \n",
    "  return summaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
